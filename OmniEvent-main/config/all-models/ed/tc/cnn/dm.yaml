seed: 2
do_train: True
do_predict: True 
do_ED_infer: True

# top-level config #
paradigm: token_classification
task_name: ED
dataset_name: ACE2005-en
language: English
test_exists_labels: True
consider_event_type: True

# file path # 
output_dir: output/ACE2005-en
type2id_path: ../../data/processed/ace2005-en/label2id.json
train_file: ../../data/processed/ace2005-en/train.unified.jsonl
validation_file: ../../data/processed/ace2005-en/valid.unified.jsonl
test_file: ../../data/processed/ace2005-en/test.unified.jsonl
insert_marker: False

# config for data processor #
return_token_type_ids: True 
truncate_seq2seq_output: False

# model config #
model_type: cnn
model_name_or_path: cnn 
vocab_file: ../../data/wordvec/glove-100d
word_embedding_dim: 100
position_embedding_dim: 5
hidden_dropout_prob: 0.5
hidden_size: 200
aggregation: dynamic_pooling
head_scale: 2
dropout_after_wordvec: True
early_stopping_patience: 20

# training config #
num_train_epochs: 20
max_seq_length: 160
max_out_length: 160
dataloader_num_workers: 2

per_device_train_batch_size: 170
per_device_eval_batch_size: 170
gradient_accumulation_steps: 1
# eval_accumulation_steps: 4
learning_rate: 1.0e-3
weight_decay: 1.0e-8
warmup_ratio: 0.1
# lr_scheduler_type: constant
max_grad_norm: 1
optim: adamw_torch

load_best_model_at_end: True
metric_for_best_model: micro_f1 
greater_is_better: True 

logging_strategy: steps
logging_steps: 100
evaluation_strategy: epoch
eval_steps: 500
save_strategy: epoch
save_steps: 500

# split inference #
split_infer: False 
split_infer_size: 500


